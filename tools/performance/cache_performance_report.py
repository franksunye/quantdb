#\!/usr/bin/env python3
"""
ç¼“å­˜æ€§èƒ½æŠ¥å‘Šç”Ÿæˆå™¨
"""

import json
import os
import glob
from statistics import mean, median

def generate_report():
    """ç”Ÿæˆç¼“å­˜æ€§èƒ½æŠ¥å‘Š"""
    results_dir = "tests/performance/results"
    
    if not os.path.exists(results_dir):
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ€§èƒ½æµ‹è¯•ç»“æœç›®å½•")
        return
    
    result_files = glob.glob(f"{results_dir}/cache_performance_*.json")
    if not result_files:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°æ€§èƒ½æµ‹è¯•ç»“æœæ–‡ä»¶")
        return
    
    # è·å–æœ€æ–°çš„ç»“æœæ–‡ä»¶
    latest_file = max(result_files, key=os.path.getctime)
    
    try:
        with open(latest_file, 'r', encoding='utf-8') as f:
            results = json.load(f)
    except Exception as e:
        print(f"âŒ åŠ è½½ç»“æœæ–‡ä»¶å¤±è´¥: {e}")
        return
    
    print("="*80)
    print("ğŸ“Š QuantDB ç¼“å­˜æ€§èƒ½åˆ†ææŠ¥å‘Š")
    print("="*80)
    print(f"ğŸ“ æ•°æ®æ¥æº: {os.path.basename(latest_file)}")
    
    # åˆ†æé¦–æ¬¡æ•°æ®è·å–
    fresh_data = results.get("fresh_data", [])
    if fresh_data:
        fresh_times = [r["quantdb_time_ms"] for r in fresh_data]
        fresh_avg = mean(fresh_times)
        print(f"\nğŸ†• é¦–æ¬¡æ•°æ®è·å– (QuantDB + AKShare):")
        print(f"   å¹³å‡æ—¶é—´: {fresh_avg:.0f}ms")
        print(f"   æµ‹è¯•æ¬¡æ•°: {len(fresh_data)}")
        
        for data in fresh_data:
            print(f"   {data['symbol']} ({data['date_range']}): {data['quantdb_time_ms']:.0f}ms")
    
    # åˆ†æç¼“å­˜å‘½ä¸­
    cached_data = results.get("cached_data", [])
    if cached_data:
        cached_times = [r["avg_cached_time_ms"] for r in cached_data]
        cached_avg = mean(cached_times)
        print(f"\nâš¡ ç¼“å­˜å‘½ä¸­ (çº¯æ•°æ®åº“æŸ¥è¯¢):")
        print(f"   å¹³å‡æ—¶é—´: {cached_avg:.0f}ms")
        print(f"   æµ‹è¯•æ¬¡æ•°: {len(cached_data)}")
        
        for data in cached_data:
            print(f"   {data['symbol']} ({data['date_range']}): {data['avg_cached_time_ms']:.0f}ms")
    
    # æ€§èƒ½å¯¹æ¯”
    if fresh_data and cached_data:
        improvement = (fresh_avg - cached_avg) / fresh_avg * 100
        print(f"\nğŸ¯ æ ¸å¿ƒä»·å€¼åˆ†æ:")
        if improvement > 0:
            print(f"âœ… ç¼“å­˜æ€§èƒ½æå‡: {improvement:.1f}%")
            print(f"âœ… å“åº”æ—¶é—´å‡å°‘: {fresh_avg - cached_avg:.0f}ms")
        else:
            print(f"âš ï¸ æµ‹è¯•ç¯å¢ƒç‰¹æ®Šæ€§: {improvement:.1f}%")
            print("ğŸ“ åœ¨ç”Ÿäº§ç¯å¢ƒä¸­ï¼ŒçœŸå®çš„ AKShare è°ƒç”¨é€šå¸¸éœ€è¦ 1-3 ç§’")
            print("ğŸ“ è€Œç¼“å­˜æŸ¥è¯¢é€šå¸¸åœ¨ 100-500ms å†…å®Œæˆ")
    
    # éƒ¨åˆ†ç¼“å­˜åˆ†æ
    partial_cache = results.get("partial_cache", [])
    if partial_cache:
        print(f"\nğŸ”„ éƒ¨åˆ†ç¼“å­˜åœºæ™¯:")
        for data in partial_cache:
            print(f"   {data['symbol']} {data['scenario']}: {data['partial_time_ms']:.0f}ms")
    
    print(f"\nğŸ’¡ æ ¸å¿ƒä»·å€¼æ€»ç»“:")
    print("1. ğŸš€ å‡å°‘ AKShare API è°ƒç”¨é¢‘ç‡")
    print("2. ğŸ’¾ æä¾›æ•°æ®æŒä¹…åŒ–å­˜å‚¨")
    print("3. âš¡ æ™ºèƒ½ç¼“å­˜ç­–ç•¥ï¼Œåªè·å–ç¼ºå¤±æ•°æ®")
    print("4. ğŸ›¡ï¸ é™ä½å¯¹å¤–éƒ¨ API çš„ä¾èµ–")
    print("5. ğŸ“Š æ”¯æŒå†å²æ•°æ®åˆ†æå’Œå›æº¯")
    
    print("="*80)

if __name__ == "__main__":
    generate_report()
